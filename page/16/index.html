<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content=""><meta name="keywords" content=""><meta name="author" content="Joe Huang"><meta name="copyright" content="Joe Huang"><title>Awaken Desparado</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.6.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.1"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script><meta name="generator" content="Hexo 4.1.1"></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="false"><div class="author-info"><div class="author-info__avatar text-center"><img src="/img/avatar.png"></div><div class="author-info__name text-center">Joe Huang</div><div class="author-info__description text-center"></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">189</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">24</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">21</span></a></div></div></div><nav class="no-bg" id="nav"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">Awaken Desparado</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span><span class="pull-right"></span></div><div id="site-info"><div id="site-title">Awaken Desparado</div><div id="site-sub-title"></div></div></nav><div id="content-outer"><div class="layout" id="content-inner"><div class="recent-post-item article-container"><a class="article-title" href="/2019/09/11/OpenGL/2019-09-11-OpenGL%20-%20shader%202&amp;3&amp;4&amp;5/">OpenGL/2019-09-11-OpenGL - shader 2&amp;3&amp;4&amp;5</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-09-11</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/OpenGL/">OpenGL</a></span><div class="content"><h3 id=""><a href="#" class="headerlink" title=""></a></h3><h3 id="OpenGL-shader-2"><a href="#OpenGL-shader-2" class="headerlink" title="OpenGL - shader 2"></a>OpenGL - shader 2</h3><p><img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g7fb0l120zj312k0fdgzh.jpg" alt="image-20190928161919955"></p>
<p>![image-20190928162048236](/Users/joe/Library/Application Support/typora-user-images/image-20190928162048236.png)</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g7fb7mqsisj30qe097tgb.jpg" alt="image-20190928162610910"></p>
<h3 id="OpenGL-shader3"><a href="#OpenGL-shader3" class="headerlink" title="OpenGL - shader3"></a>OpenGL - shader3</h3><p><img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g7gev3sadrj30of0pqk4p.jpg" alt="image-20190929151757208"></p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g7gevyp1egj30ij0ojwj6.jpg" alt="image-20190929151855171"></p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g7gex435gtj30g60nan19.jpg" alt="image-20190929152001763"></p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g7gf2uiqehj30gw02g74y.jpg" alt="image-20190929152532403"></p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g7gfamm1epj30nx02tmy4.jpg" alt="image-20190929153106469"></p>
<h3 id="OpenGL-Shader4-FileStream"><a href="#OpenGL-Shader4-FileStream" class="headerlink" title="OpenGL - Shader4 FileStream"></a>OpenGL - Shader4 FileStream</h3><p>Disk - Sector &gt; Track</p>
<p>Sector as unit to return to memory, </p>
<h4 id="FileBuffer-for-CPU-to-handle"><a href="#FileBuffer-for-CPU-to-handle" class="headerlink" title="FileBuffer for CPU to handle"></a>FileBuffer for CPU to handle</h4><h4 id="StringBuffer-to-intepret-FileB"><a href="#StringBuffer-to-intepret-FileB" class="headerlink" title="StringBuffer to intepret FileB"></a>StringBuffer to intepret FileB</h4><h4 id="CPU-to-read-char"><a href="#CPU-to-read-char" class="headerlink" title="CPU to read char*"></a>CPU to read char*</h4><p><img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g7gfgkdj9dj30mo0diq78.jpg" alt="image-20190929153841303"></p>
<h3 id="Coding"><a href="#Coding" class="headerlink" title="Coding"></a>Coding</h3><p><img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g7gfkbettaj30h40fcdl2.jpg" alt="image-20190929154217227"></p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g7q3asksbgj30me0m47gn.jpg" alt="image-20191008001345211"></p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2019/09/11/OpenGL/2019-09-11-OpenGL%201/">OpenGL/2019-09-11-OpenGL 1</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-09-11</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/OpenGL/">OpenGL</a></span><div class="content"><h3 id="GLSL"><a href="#GLSL" class="headerlink" title="GLSL"></a>GLSL</h3><p>让程式码知道时间的概念</p>
<p>CPU用uniform去处理时间概念</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g7bvl5xl9cj30wm0jnds2.jpg" alt="image-20190925170858998"></p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g7bvmvkicgj30lu046abv.jpg" alt="image-20190925171031074"></p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g7bvx90nunj30l309k78m.jpg" alt="image-20190925172036044"></p>
<p>↓</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g7bvy23gurj30m70ex0zf.jpg" alt="image-20190925172119986"></p>
<h3 id="Uniform"><a href="#Uniform" class="headerlink" title="Uniform"></a>Uniform</h3><p><img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g7bw7nkgf8j30nz045q6o.jpg" alt="image-20190925173034574"></p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2019/09/10/OpenGL/2019-09-10-OpenGL-3D-Triangle-VBO-VAO/">OpenGL/2019-09-10-OpenGL-3D-Triangle-VBO-VAO</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-09-10</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/OpenGL/">OpenGL</a></span><div class="content"><h3 id="3D-2D"><a href="#3D-2D" class="headerlink" title="3D 2D"></a>3D 2D</h3><p>All 3D space concepts in OpenGL.</p>
<p>3D coordinates (camera) –&gt; 2D plane –&gt; colored pixels.</p>
<p><strong><em>2D坐标和像素也是不同的，2D坐标精确表示一个点在2D空间中的位置，而2D像素是这个点的近似值，2D像素受到你的屏幕/窗口分辨率的限制。</em></strong></p>
<h3 id="Shader"><a href="#Shader" class="headerlink" title="Shader"></a>Shader</h3><p>– program that GPU runs</p>
<p>有些着色器允许开发者自己配置，这就允许我们用自己写的着色器来替换默认的。这样我们就可以更细致地控制图形渲染管线中的特定部分了，而且因为它们运行在GPU上，所以它们可以给我们节约宝贵的CPU时间。OpenGL着色器是用OpenGL着色器语言(OpenGL Shading Language, GLSL)写成的，在下一节中我们再花更多时间研究它。</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g72v7ypj0gj30uk0ian1n.jpg" alt="image-20190917220604821"></p>
<p>CPU –&gt; GPU</p>
<p>必用的 ：Vertex Shader, Fragment Shader</p>
<h4 id="Geometry-Shader"><a href="#Geometry-Shader" class="headerlink" title="Geometry Shader"></a>Geometry Shader</h4><p>可以偷補插點</p>
<h4 id="Vertex"><a href="#Vertex" class="headerlink" title="Vertex"></a>Vertex</h4><p>我们以数组的形式传递3个3D坐标作为图形渲染管线的输入，用来表示一个三角形，这个数组叫做顶点数据(Vertex Data)；顶点数据是一系列顶点的集合。一个顶点(Vertex)是一个3D坐标的数据的集合。而顶点数据是用顶点属性(Vertex Attribute)表示的，它可以包含任何我们想用的数据，但是简单起见，我们还是假定每个顶点只由一个3D位置和一些颜色值、UV组成。</p>
<p><strong><em>为了让OpenGL知道我们的坐标和颜色值构成的到底是什么，OpenGL需要你去指定这些数据所表示的渲染类型。我们是希望把这些数据渲染成一系列的点？一系列的三角形？还是仅仅是一个长长的线？做出的这些提示叫做图元(Primitive)，任何一个绘制指令的调用都将把图元传递给OpenGL。这是其中的几个：GL_POINTS、GL_TRIANGLES、GL_LINE_STRIP。</em></strong></p>
<h3 id="———"><a href="#———" class="headerlink" title="———"></a>———</h3><p>VertexShader -&gt; Rasterization -&gt; FragmentShader</p>
<h3 id="Normalized-Device-Coordinates-NDC"><a href="#Normalized-Device-Coordinates-NDC" class="headerlink" title="Normalized Device Coordinates (NDC)"></a>Normalized Device Coordinates (NDC)</h3><p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g74lnj2v91j30wq0i0dv3.jpg" alt="image-20190919100557092"></p>
<h3 id="Vertex-Data"><a href="#Vertex-Data" class="headerlink" title="Vertex Data"></a>Vertex Data</h3><p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g74m3bcwprj30uq0f4gss.jpg" alt="image-20190919102118610"></p>
<p>f: means 法向量</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g74m6drrucj31b30u0wyu.jpg" alt="image-20190919102416139">   (in Blender)</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g74m5tdw1hj30xi0j2jzt.jpg" alt="image-20190919102343448"></p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g74m7xnz39j31c40q4dp7.jpg" alt="image-20190919102545255"></p>
<p>三角化</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g74m927ogwj30kk0diwfb.jpg" alt="image-20190919102640344"></p>
<h3 id="VBO-amp-VAO"><a href="#VBO-amp-VAO" class="headerlink" title="VBO &amp; VAO"></a>VBO &amp; VAO</h3><p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g74v9xvqztj30x60hedpb.jpg" alt="image-20190919153900923"></p>
<h3 id="UV"><a href="#UV" class="headerlink" title="UV"></a>UV</h3><p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g74vebr40mj30h80c2447.jpg" alt="image-20190919154317764"></p>
<h3 id="———–"><a href="#———–" class="headerlink" title="———–"></a>———–</h3><h3 id="EBO"><a href="#EBO" class="headerlink" title="EBO"></a>EBO</h3><p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g74vtx1bshj30w60i4mzf.jpg" alt="image-20190919155433545"></p>
<h3 id="VAO"><a href="#VAO" class="headerlink" title="VAO"></a>VAO</h3><p>一個物件一個VAO</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">unsigned</span> <span class="keyword">int</span> VAO;</span><br><span class="line">glGenVertexArrays(<span class="number">1</span>, &amp;VAO);</span><br><span class="line"></span><br><span class="line">--------P.S.--------------------</span><br><span class="line"><span class="keyword">unsigned</span> <span class="keyword">int</span> VAO[<span class="number">10</span>]; <span class="comment">//  一次可造多個 array</span></span><br><span class="line">glGenVertexArrays(<span class="number">10</span>, VAO);</span><br><span class="line">--------P.S.--------------------</span><br><span class="line">  </span><br><span class="line">glBindVertexArray(VAO);</span><br><span class="line"></span><br><span class="line"><span class="keyword">unsigned</span> <span class="keyword">int</span> VBO;</span><br><span class="line">glBindBuffer(GL_ARRAY_BUFFER, VBO);</span><br><span class="line">glBufferData(GL_ARRAY_BUFFER, <span class="keyword">sizeof</span>(vertices), vertices, GL_STATIC_DRAW);</span><br></pre></td></tr></table></figure>

<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g74wk2zb4dj30i304j41v.jpg" alt="image-20190919162325003"></p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2019/09/06/CV%20notes/2019-09-06-Face-Keypoint-Detection/">CV notes/2019-09-06-Face-Keypoint-Detection</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-09-06</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Deep-Learning/">Deep-Learning</a><i class="fa fa-angle-right" aria-hidden="true"></i><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Deep-Learning/AI/">AI</a></span><div class="content"><p>Ref from: <a href="https://niuyuanyuanna.github.io/2018/11/08/computer_version/face-keypoint-detection/" target="_blank" rel="noopener">https://niuyuanyuanna.github.io/2018/11/08/computer_version/face-keypoint-detection/</a></p>
<h1 id="人脸关键点检测"><a href="#人脸关键点检测" class="headerlink" title="人脸关键点检测"></a>人脸关键点检测</h1><p>关于人脸识别和表情分类的一些<a href="https://zhuanlan.zhihu.com/p/31638581" target="_blank" rel="noopener">论文</a></p>
<h2 id="人脸关键点检测介绍"><a href="#人脸关键点检测介绍" class="headerlink" title="人脸关键点检测介绍"></a>人脸关键点检测介绍</h2><p>人脸关键点检测也称为人脸关键点检测、定位或者人脸对齐，是指给定人脸图像，定位出人脸面部的关键区域位置，包括眉毛、眼睛、鼻子、嘴巴、脸部轮廓等。</p>
<p>关键点的集合称作形状(shape)，形状包含了关键点的位置信息，而这个位置信息一般可以用两种形式表示，第一种是关键点的位置相对于整张图像，第二种是关键点的位置相对于人脸框(标识出人脸在整个图像中的位置)。把第一种形状称作绝对形状，它的取值一般介于 [0∼h][0∼h]或[0∼w][0∼w]，第二种形状我们称作相对形状，它的取值一般介于 0 到 1。这两种形状可以通过人脸框转换。</p>
<h3 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h3><p>人脸关键点检测分为三种：</p>
<ul>
<li>基于ASM(Active Shape Model)和AAM (Active Appearnce Model) 的传统方法、参数化方法</li>
<li>基于CSR(Cascaded Shape Regression)的方法、非参数化方法</li>
<li>基于深度学习、非参数化方法</li>
</ul>
<p>基于参数化形状模型的方法可依据其外观模型的不同，可进一步分为，基于局部的方法和基于全局的方法；对于非参数化进一步可分为基于样例的方法、基于图模型的方法、基于级联回归的方法和基于深度学习的方法。</p>
<h3 id="人脸关键点评价标准"><a href="#人脸关键点评价标准" class="headerlink" title="人脸关键点评价标准"></a>人脸关键点评价标准</h3><p>目前主要的衡量标准是算法所获取的关键点位置与真实关键点位置之间的偏差。在评价偏差时，由于不同人脸图像的实际大小难免会有所差异，为便于在同样的尺度下比较算法性能，需要采用一定的数据归一化策略。 目前主流的方法是基于两眼间的距离进行人脸大小的标准化：<br>$$<br>ex=∣∣x^−xGT∣∣DIODex=DIOD∣∣x^−xGT∣∣<br>$$<br>其中分子DIODDIOD表示估计值与真实值的欧式距离，分母∣∣x^−xGT∣∣∣∣x^−xGT∣∣表示双眼距离，即两眼中心的欧式距离。也有采用边界框对角线作为归一化因子来评价偏差。</p>
<h3 id="人脸常用数据库"><a href="#人脸常用数据库" class="headerlink" title="人脸常用数据库"></a>人脸常用数据库</h3><p>数据库可以分为两类：主动式捕获的数据和被动式捕获的数据。主动式捕获的数据是在实验室里，对光照变化、遮挡、头部姿态和面部表情可控的情况下，对固定人员进行照片采集。被动式捕获的数据则是在社交网站等一些环境不可控的条件下采集而得。</p>
<ul>
<li>主动式数据</li>
</ul>
<ol>
<li>CMU Multi-PIE：在2004年10月至2005年3月的四次会议中收集的，支持在姿态、光照和表情变化条件下识别人脸的算法的开发。 该数据库包含337个主题和超过750,000个305GB数据的图像。 共记录了六种不同的表情：中性，微笑，惊奇，斜视，厌恶和尖叫。 在15个视图和19个不同照明条件下记录受试者，这个数据库的一个子集被标记为68点或39点。</li>
<li>XM2VTS：收集了295人的2360个彩色图像，声音文件和3D人脸模型，这2360个彩色图像标有68个关键点。</li>
<li>AR：包含超过4000个彩色图像，对应126人（70名男性和56名女性）的脸部。图像是在可控的条件下，以不同的面部表情，光照条件和遮挡（太阳镜和围巾）拍摄的。Ding and Martinez手动为每张脸部图像标注了130个关键点。</li>
<li>IMM：包含240张40个人的彩色图像（7名女性和33名男性）。 每张图像都对眉毛、眼睛、鼻子、嘴巴和下巴进行标注，共计58个标记点。</li>
<li>MUCT：由276个人的3755张图像组成，每张图像有76个关键点。 这个数据库中的面孔在不同的光照、不同的年龄和不同的种族的条件下拍摄。</li>
<li>PUT：采集了部分光照条件可控的100个人，且沿着俯仰角和偏航角旋转的9971张高分辨率图像（2048×1536），每张图像都标有30个关键点。</li>
</ol>
<ul>
<li>被动式数据</li>
</ul>
<ol>
<li>BioID：记录在室内实验室环境中，但使用“真实世界”的条件。 该数据库包含23个主题的1521个灰度人脸图像，每张图像标记20个关键点。</li>
<li>LFW：包含从网上收集的5724个人的13,233幅面部图像，其中1680人在数据集中有两张或更多的照片。虽然，这个数据库没有提供标记点，但可以从其余网站上获取。</li>
<li>AFLW(Annotated Facial Landmarks in the Wild) ：是一个大规模、多视角和真实环境下的人脸数据库。图像是从图片分享网站Flickr上收集，该数据库共包含25,993张图像，每张图像标有21个关键点。</li>
<li>LFPW(Labeled Face Parts in the Wild)：由1400个面部图像（1100作为训练集，其他300个图像作为测试集）组成。所有数据均从google, Flickr和Yahoo上获取，每张图像标记35个关键点，但在文献中，通常采用29个关键点。</li>
<li>AFW(Annotated Faces in the Wild)：包含205个图像，特点是：背景高度混乱，人脸比例和姿势都有很大的变化，每张图像均有6个关键点和边界框。</li>
<li>300-W(300 Faces in-the-Wild Challenge)：一个混合数据库，由多个已发布数据库（LFPW，Helen，AFW和XM2VTS）的面部图像和一个新收集的数据库IBUG组成。 所有这些图像都重新标注了68个关键点。</li>
</ol>
<h3 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h3><h4 id="ASM检测方法"><a href="#ASM检测方法" class="headerlink" title="ASM检测方法"></a>ASM检测方法</h4><p>ASM(Active Shape Model)是由Cootes于1995年提出的经典的人脸关键点检测算法，主动形状模型即通过形状模型对目标物体进行抽象，ASM是一种基于点分布模型（Point Distribution Model, PDM）的算法。在PDM中，外形相似的物体，例如人脸、人手、心脏、肺部等的几何形状可以通过若干关键点（landmarks）的坐标依次串联形成一个形状向量来表示。ASM算法需要通过人工标定的方法先标定训练集，经过训练获得形状模型，再通过关键点的匹配实现特定物体的匹配。</p>
<p>其检测过程主要分为两步：</p>
<ul>
<li>训练<ul>
<li>图像预处理：<ul>
<li>搜集n个训练样本（n=400）， 手动标记脸部关键点；</li>
<li>将训练集中关键点的坐标串成特征向量</li>
<li>对形状进行归一化和对齐（对齐采用Procrustes方法）</li>
<li>对齐后的形状特征做PCA处理</li>
</ul>
</li>
<li>为关键点构建局部特征（在每次迭代搜索过程中每个关键点可以寻找新的位置）：<ul>
<li>局部特征一般用梯度特征，以防光照变化</li>
<li>有的方法沿着边缘的法线方向提取，有的方法在关键点附近的矩形区域提取</li>
</ul>
</li>
</ul>
</li>
<li>搜索<ul>
<li>计算眼睛（或者眼睛和嘴巴）的位置，做简单的尺度和旋转变化，对齐人脸</li>
<li>在对齐后的各个点附近搜索，匹配每个局部关键点（常采用马氏距离），得到初步形状</li>
<li>用平均人脸（形状模型）修正匹配结果</li>
<li>迭代直到收敛</li>
</ul>
</li>
</ul>
<p>优点：模型简单直接，架构清晰明确，易于理解和应用，而且对轮廓形状有着较强的约束</p>
<p>缺点：其近似于穷举搜索的关键点定位方式在一定程度上限制了其运算效率</p>
<h4 id="AAM"><a href="#AAM" class="headerlink" title="AAM"></a>AAM</h4><p>AAM（Active Appearance Models）。1998年，Cootes对ASM进行改进，不仅采用形状约束，而且又加入整个脸部区域的纹理特征，提出了AAM算法[2]。AAM于ASM一样，主要分为两个阶段，模型建立阶段和模型匹配阶段。其中模型建立阶段包括对训练样本分别建立形状模型(Shape Model)和纹理模型(Texture Model)，然后将两个模型进行结合，形成AAM模型。</p>
<h4 id="CPR"><a href="#CPR" class="headerlink" title="CPR"></a>CPR</h4><p>2010年，Dollar提出CPR（Cascaded Pose Regression, 级联姿势回归），CPR通过一系列回归器将一个指定的初始预测值逐步细化，每一个回归器都依靠前一个回归器的输出来执行简单的图像操作，整个系统可自动的从训练样本中学习。<br>人脸关键点检测的目的是估计向量</p>
<p>S=(x1,x2,⋅⋅⋅,xk,⋅⋅⋅,xK)∈R2KS=(x1,x2,⋅⋅⋅,xk,⋅⋅⋅,xK)∈R2K</p>
<p>其中K表示关键点的个数，由于每个关键点有横纵两个坐标，所以S的长度为2K。CPR检测一共有T个阶段，在每个阶段中首先进行特征提取，得到</p>
<p>ft=ϕ(I,St)ft=ϕ(I,St)</p>
<p>这里特征使用的是shape-indexed features，也可以使用诸如HOG、SIFT等人工设计的特征，或者其他可学习特征（learning based features），然后通过训练得到的回归器rtrt来估计增量ΔSΔS( update vector)</p>
<p>ΔS=rt(ϕ(I,St))ΔS=rt(ϕ(I,St))</p>
<p>把ΔSΔS加到前一个阶段的SS上得到新的SS，这样通过不断的迭代即可以得到最终的S(shape)。</p>
<p>St+1=St+ΔSSt+1=St+ΔS</p>
<h4 id="DCNN"><a href="#DCNN" class="headerlink" title="DCNN"></a>DCNN</h4><p>2013 年，Sun 等人 首次将 CNN 应用到人脸关键点检测，提出一种级联的 CNN（拥有三个层级）——DCNN(Deep Convolutional Network)，此种方法属于级联回归方法。作者通过精心设计拥有三个层级的级联卷积神经网络，不仅改善初始不当导致陷入局部最优的问题，而且借助于 CNN 强大的特征提取能力，获得更为精准的关键点检测。</p>
<p><a href="https://github.com/niuyuanyuanna/BlogImages/raw/master/computerVersion/06230452.png" target="_blank" rel="noopener"><img src="https://github.com/niuyuanyuanna/BlogImages/raw/master/computerVersion/06230452.png" alt="img"></a></p>
<p>如上图所示，DCNN由三个level构成。level-1是3个CNN网络；level-2是10个CNN块构成，每个关键点采用两个CNN；level-3结构类似于level-2，都为10个CNN。</p>
<p>Level-1的三个CNN分别为 F1（Face 1）、EN1（Eye，Nose）、NM1（Nose，Mouth）。F1 输入尺寸为 39<em>39，输出 5 个关键点的坐标；EN1 输入尺寸为 39</em>31，输出是 3 个关键点的坐标；NM11 输入尺寸为 39*31，输出是 3 个关键点；Level-1 的输出为三个 CNN 输出取平均。</p>
<p>Level-2，由 10 个 CNN 构成，输入尺寸均为 15*15，每两个组成一对，一对 CNN 对一个关键点进行预测，预测结果同样是采取平均。</p>
<p>Level-3 与 Level-2 一样，由 10 个 CNN 构成，输入尺寸均为 15*15，每两个组成一对。Level-2 和 Level-3 是对 Level-1 得到的粗定位进行微调，得到精细的关键点定位。</p>
<p>Level-1 之所以比 Level-2 和 Level-3 的输入要大，是因为作者认为，由于人脸检测器的原因，边界框的相对位置可能会在大范围内变化，再加上面部姿态的变化，最终导致输入图像的多样性，因此在 Level-1 应该需要有足够大的输入尺寸。Level-1 与 Level-2 和 Level-3 还有一点不同之处在于，Level-1 采用的是局部权值共享（Lcally Sharing Weights），作者认为传统的全局权值共享是考虑到，某一特征可能在图像中任何位置出现，所以采用全局权值共享。然而，对于类似人脸这样具有固定空间结构的图像而言，全局权值共享就不奏效了。因为眼睛就是在上面，鼻子就是在中间，嘴巴就是在下面的。所以作者采用局部权值共享，通过实验证明了局部权值共享给网络带来性能提升。</p>
<p>DCNN 采用级联回归的思想，从粗到精的逐步得到精确的关键点位置，不仅设计了三级级联的卷积神经网络，还引入局部权值共享机制，从而提升网络的定位性能。最终在数据集 BioID 和 LFPW 上均获得当时最优结果。速度方面，采用 3.3GHz 的 CPU，每 0.12 秒检测一张图片的 5 个关键点。</p>
<h4 id="Face-DCNN"><a href="#Face-DCNN" class="headerlink" title="Face++ DCNN"></a>Face++ DCNN</h4><p>2013 年，Face++在 DCNN 模型上进行改进，提出从粗到精的人脸关键点检测算法，实现了 68 个人脸关键点的高精度定位。该算法将人脸关键点分为内部关键点和轮廓关键点，内部关键点包含眉毛、眼睛、鼻子、嘴巴共计 51 个关键点，轮廓关键点包含 17 个关键点。</p>
<p>针对内部关键点和外部关键点，该算法并行的采用两个级联的 CNN 进行关键点检测，网络结构如图所示：</p>
<p><a href="https://github.com/niuyuanyuanna/BlogImages/raw/master/computerVersion/92897228.jpg" target="_blank" rel="noopener"><img src="https://github.com/niuyuanyuanna/BlogImages/raw/master/computerVersion/92897228.jpg" alt="img"></a></p>
<p>针对内部 51 个关键点，采用四个层级的级联网络进行检测。其中，Level-1 主要作用是获得面部器官的边界框；Level-2 的输出是 51 个关键点预测位置，这里起到一个粗定位作用，目的是为了给 Level-3 进行初始化；Level-3 会依据不同器官进行从粗到精的定位；Level-4 的输入是将 Level-3 的输出进行一定的旋转，最终将 51 个关键点的位置进行输出。针对外部 17 个关键点，仅采用两个层级的级联网络进行检测。Level-1 与内部关键点检测的作用一样，主要是获得轮廓的 bounding box；Level-2 直接预测 17 个关键点，没有从粗到精定位的过程，因为轮廓关键点的区域较大，若加上 Level-3 和 Level-4，会比较耗时间。最终面部 68 个关键点由两个级联 CNN 的输出进行叠加得到。</p>
<p>算法创新点：</p>
<ol>
<li>把人脸的关键点定位问题，划分为内部关键点和轮廓关键点分开预测，有效的避免了 loss 不均衡问题</li>
<li>在内部关键点检测部分，并采用 DCNN 的方法，将每个关键点采用两个 CNN 进行预测，而是每个器官采用一个 CNN 进行预测，从而减少计算量</li>
<li>相比于 DCNN，没有直接采用人脸检测器返回的结果作为输入，而是增加一个边界框检测层（Level-1），可以大大提高关键点粗定位网络的精度。</li>
</ol>
<h4 id="TCDNN"><a href="#TCDNN" class="headerlink" title="TCDNN"></a>TCDNN</h4><p>2014 年，Zhang 等人将 MTL（Multi-Task Learning）应用到人脸关键点检测中，提出 TCDCN（Tasks-Constrained Deep Convolutional Network）。作者认为，在进行人脸关键点检测任务时，结合一些辅助信息可以帮助更好的定位关键点，这些信息如，性别、是否带眼镜、是否微笑和脸部的姿势等等。作者将人脸关键点检测（5 个关键点）与性别、是否带眼镜、是否微笑及脸部的姿势这四个子任务结合起来构成一个多任务学习模型，模型框架如图所示。</p>
<p><a href="https://github.com/niuyuanyuanna/BlogImages/raw/master/computerVersion/15813947.jpg" target="_blank" rel="noopener"><img src="https://github.com/niuyuanyuanna/BlogImages/raw/master/computerVersion/15813947.jpg" alt="img"></a></p>
<p>网络输入为40×40的灰度图，通过CNN后得到2×2×64的特征图，再通过一层含有100个神经元的全连接层输出最终提取到的共享特征。该特征为所有任务共享，对于关键点检测问题，就采用线性回归模型；对于分类问题，就采用逻辑回归。</p>
<p>在传统 MLT 中，各任务重要程度是一致的，其目标方程如下：</p>
<p>arg⁡min⁡∑t−1T∑i=1Nℓ(yit,f(xit;wt))+Φ(wt){wt}argmint−1∑Ti=1∑Nℓ(yit,f(xit;wt))+Φ(wt){wt}t=1T</p>
<p>其中，</p>
<ul>
<li>f(xit;wt)f(xit;wt)表示输入矩阵xitxit与权值矩阵$ \mathbf{w}^t$运算后得到的输出</li>
<li>ℓ(.)ℓ(.)表示损失函数</li>
<li>Φ(wt)Φ(wt)表示正则化</li>
</ul>
<p>对于各任务 t 而言，其重要性是相同的，但是在多任务学习中，往往不同任务的学习难易程度不同，若采用相同的损失权重，会导致学习任务难以收敛。文章针对多任务学习中，不同学习难度问题进行了优化，提出带权值的目标函数：</p>
<p>wr,{wa}  ∑i=1Nℓ(yit,f(xit;wt))+∑i=1N∑a∈Aλaℓ(yia,f(xia;wa))argminwr,{wa}a∈A  i=1∑Nℓ(yit,f(xit;wt))+i=1∑Na∈A∑λaℓ(yia,f(xia;wa))</p>
<p>式中前一项为人脸关键点检测的损失函数，第二项表示其他任务的损失函数，λaλa为任务aa的重要程度。在论文中，四个子任务分别为：性别、是否带眼镜、微笑、脸部姿势，因此，优化目标函数为：</p>
<p>wr,{wa}  12∑i=1N∥(yit,f(xit;wt)∥2+∑i=1N∑a∈Aλayialog⁡(p(yia∣xia;wa))+∑t=1T∥wa∥22argminwr,{wa}a∈A  21i=1∑N∥(yit,f(xit;wt)∥2+i=1∑Na∈A∑λayialog(p(yia∣xia;wa))+t=1∑T∥wa∥22</p>
<p>分类任务采用交叉熵损失函数。</p>
<p>针对多任务学习的另外一个问题——各任务收敛速度不同，本文提出一种新的提前停止（Early Stopping）方法。当某个子任务达到最好表现以后，这个子任务就对主任务已经没有帮助，就可以停止这个任务。</p>
<p>TCDCN 采用多任务学习方法对人脸关键点进行检测，针对多任务学习在人脸关键点检测任务中的两个主要问题：不同任务学习难易程度不同以及不同任务收敛速度不同，分别提出了新目标函数和提前停止策略加以改进，最终在 AFLW 和 AFW 数据集上获得领先的结果。同时对比于级联 CNN 方法，在 Intel Core i5 cpu 上，级联 CNN 需要 0.12s，而 TCDCN 仅需要 17ms，速度提升七倍有余。</p>
<h4 id="MTCNN"><a href="#MTCNN" class="headerlink" title="MTCNN"></a>MTCNN</h4><p>2016 年，Zhang 等人提出一种多任务级联卷积神经网络（MTCNN, Multi-task Cascaded Convolutional Networks）用以同时处理人脸检测和人脸关键点定位问题。作者认为人脸检测和人脸关键点检测两个任务之间往往存在着潜在的联系，然而大多数方法都未将两个任务有效的结合起来，本文为了充分利用两任务之间潜在的联系，提出一种多任务级联的人脸检测框架，将人脸检测和人脸关键点检测同时进行。</p>
<p>MTCNN 包含三个级联的多任务卷积神经网络，分别是 Proposal Network (P-Net)、Refine Network (R-Net)、Output Network (O-Net)，每个多任务卷积神经网络均有三个学习任务，分别是人脸分类、边框回归和关键点定位。网络结构如图所示：</p>
<p><a href="https://github.com/niuyuanyuanna/BlogImages/raw/master/computerVersion/14009657.jpg" target="_blank" rel="noopener"><img src="https://github.com/niuyuanyuanna/BlogImages/raw/master/computerVersion/14009657.jpg" alt="img"></a></p>
<p>MTCNN 实现人脸检测和关键点定位分为三个阶段。首先由 P-Net 获得了人脸区域的候选窗口和边界框的回归向量，并用该边界框做回归，对候选窗口进行校准，然后通过非极大值抑制（NMS）来合并高度重叠的候选框。然后将 P-Net 得出的候选框作为输入，输入到 R-Net，R-Net 同样通过边界框回归和 NMS 来去掉那些 false-positive 区域，得到更为准确的候选框；最后，利用 O-Net 输出 5 个关键点的位置。</p>
<p>在具体训练过程中，作者就多任务学习的损失函数计算方式进行相应改进。在多任务学习中，当不同类型的训练图像输入到网络时，有些任务是不进行学习的，因此相应的损失应为 0。例如，当训练图像为背景（Non-face）时，边界框和关键点的 loss 应为 0，文中提供计算公式自动确定 loss 的选取，公式为：</p>
<p>min⁡∑i=1N∑j∈{det,box,landmark}αjβijLijmini=1∑Nj∈{det,box,landmark}∑αjβijLij</p>
<p>其中</p>
<ul>
<li>αjαj表示第jj个任务的重要程度，在P-Net中，αdet=1αdet=1，αbox=0.5αbox=0.5，αlandmark=0.5αlandmark=0.5；在R-Net中αdet=1αdet=1，αbox=0.5αbox=0.5，αlandmark=1αlandmark=1。在R-Net中将$\alpha_{landmark} $增大，因为需要对关键点进行检测，所以相应增大任务重要性</li>
<li>βij∈(0,1)βij∈(0,1)作为样本类型指示器</li>
</ul>
<p>为了提升网络性能，需要挑选出困难样本（Hard Sample），传统方法是通过研究训练好的模型进行挑选，而本文提出一种能在训练过程在线挑选困难样本的方法。在 mini-batch 中，对每个样本的损失进行排序，挑选前 70% 较大的损失对应的样本作为困难样本，同时在反向传播时，忽略那 30% 的样本，因为那 30% 样本对更新作用不大。</p>
<h4 id="TCNN"><a href="#TCNN" class="headerlink" title="TCNN"></a>TCNN</h4><p>TCNN（Tweaked Convolutional Neural Networks）2016 年，Wu 等人研究了 CNN 在人脸关键点定位任务中到底学习到的是什么样的特征，在采用 GMM（Gaussian Mixture Model, 混合高斯模型）对不同层的特征进行聚类分析，发现网络进行的是层次的，由粗到精的特征定位，越深层提取到的特征越能反应出人脸关键点的位置。针对这一发现，提出了 TCNN（Tweaked Convolutional Neural Networks），其网络结构如图所示：</p>
<p><a href="https://github.com/niuyuanyuanna/BlogImages/raw/master/computerVersion/89918024.jpg" target="_blank" rel="noopener"><img src="https://github.com/niuyuanyuanna/BlogImages/raw/master/computerVersion/89918024.jpg" alt="img"></a></p>
<p>左边为Vanilla CNN，针对 FC5FC5得到的特征进行 K 个类别聚类，将训练图像按照所分类别进行划分，用以训练所对应的 FC6KFC6K。测试时，图片首先经过 Vanilla CNN 提取特征，即 FC5FC5的输出。将 FC5FC5输出的特征与 K 个聚类中心进行比较，将 FC5FC5输出的特征划分至相应的类别中，然后选择与之相应的 FC6FC6进行连接，最终得到输出。</p>
<p>作者对 Vanilla CNN 中间各层特征进行聚类分析，并统计出关键点在各层之间的变化程度。越深层提取到的特征越紧密，因此越深层提取到的特征越能反应出人脸关键点的位置。作者在采用 K=64 时，对所划分簇的样本进行平均后绘图如下：</p>
<p><a href="https://github.com/niuyuanyuanna/BlogImages/raw/master/computerVersion/62921355.jpg" target="_blank" rel="noopener"><img src="https://github.com/niuyuanyuanna/BlogImages/raw/master/computerVersion/62921355.jpg" alt="img"></a></p>
<p>从图上可发现，每一个簇的样本反应了头部的某种姿态，甚至出现了表情和性别的差异。因此可推知，人脸关键点的位置常常和人脸的属性相关联。因此为了得到更准确的关键点定位，作者使用具有相似特征的图片训练对应的回归器，最终在人脸关键点检测数据集 AFLW,AFW 和 300W 上均获得当时最佳效果。</p>
<h2 id="DAN"><a href="#DAN" class="headerlink" title="DAN"></a>DAN</h2><h3 id="主要内容"><a href="#主要内容" class="headerlink" title="主要内容"></a>主要内容</h3><p>DAN是一种人脸对齐的方法，采用级联神经网络结构，充分利用人脸的全局信息，而不是局部信息，避免局部最小化。</p>
<ol>
<li>参考CSR框架，通过前向传播提取特征，训练前向传播的backbone网络得到关键点位的偏差，替代CSR中的回归器</li>
<li>用级联结构来实现CSR中的迭代</li>
<li>利用人脸所有信息T(I)T(I)、H(I)H(I)、F(I)F(I)作为输入，得到关键点偏差</li>
<li>构造级联网络结构</li>
<li>分级训练网络，每一级网络loss不收敛后再训练下一级网络</li>
</ol>
<h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p>2017 年，Kowalski 等人提出一种新的级联深度神经网络——DAN（Deep Alignment Network），以往级联神经网络输入的是图像的某一部分，与以往不同，DAN 各阶段网络的输入均为整张图片。当网络均采用整张图片作为输入时，DAN 可以有效的克服头部姿态以及初始化带来的问题，从而得到更好的检测效果。之所以 DAN 能将整张图片作为输入，是因为其加入了关键点热图（Landmark Heatmaps），关键点热图的使用是本文的主要创新点。DAN 基本框架如图所示：</p>
<p><a href="https://github.com/niuyuanyuanna/BlogImages/raw/master/computerVersion/55049944.jpg" target="_blank" rel="noopener"><img src="https://github.com/niuyuanyuanna/BlogImages/raw/master/computerVersion/55049944.jpg" alt="img"></a></p>
<p>DAN 包含多个阶段，每一个阶段含三个输入和一个输出，输入分别是被矫正过的图片、关键点热图和由全连接层生成的特征图，输出是面部形状（Face Shape）。其中，CONNECTION LAYER 的作用是将本阶段得输出进行一系列变换，生成下一阶段所需要的三个输入，具体操作如下图所示：</p>
<p><a href="https://github.com/niuyuanyuanna/BlogImages/raw/master/computerVersion/83789038.jpg" target="_blank" rel="noopener"><img src="https://github.com/niuyuanyuanna/BlogImages/raw/master/computerVersion/83789038.jpg" alt="img"></a></p>
<p>第一阶段的输入仅有原始图片和 S0S0。面部关键点的初始化即为 S0S0，是由所有关键点取平均得到，第一阶段输出 S1S1。对于第二阶段， S1S1经第一阶段的 CONNECTION LAYERS 进行转换，分别得到转换后图片 T2(I)T2(I)、 S1S1所对应的热图 H2H2和第一阶段 fc1fc1层输出，这三个正是第二阶段的输入。如此周而复始，直到最后一个阶段输出 SNSN。</p>
<p>DAN 要做的“IMAGE TRANSFORM“，就是图片矫正， DAN 对姿态变换具有很好的适应能力，或许就得益于这个“IMAGE TRANSFORM“。StSt公式为：</p>
<p>St=Tt−1(Tt(St−1)+ΔSt)St=Tt−1(Tt(St−1)+ΔSt)</p>
<p>Feed forward NN网络参数为：</p>
<table>
<thead>
<tr>
<th align="center">Name</th>
<th align="center">Shape-in</th>
<th align="center">Shape-out</th>
<th align="center">Kernel</th>
</tr>
</thead>
<tbody><tr>
<td align="center">conv1a</td>
<td align="center">112×112×1</td>
<td align="center">112×112×64</td>
<td align="center">3×3×1,1</td>
</tr>
<tr>
<td align="center">conv1b</td>
<td align="center">112×112×64</td>
<td align="center">112×112×64</td>
<td align="center">3×3×64,1</td>
</tr>
<tr>
<td align="center">pool1</td>
<td align="center">112×112×64</td>
<td align="center">56×56×64</td>
<td align="center">2×2×1,2</td>
</tr>
<tr>
<td align="center">conv2a</td>
<td align="center">56×56×64</td>
<td align="center">56×56×128</td>
<td align="center">3×3×64,1</td>
</tr>
<tr>
<td align="center">conv2b</td>
<td align="center">56×56×128</td>
<td align="center">56×56×128</td>
<td align="center">3×3×128,1</td>
</tr>
<tr>
<td align="center">pool2</td>
<td align="center">56×56×128</td>
<td align="center">28×28×128</td>
<td align="center">2×2×1,2</td>
</tr>
<tr>
<td align="center">conv3a</td>
<td align="center">28×28×128</td>
<td align="center">28×28×256</td>
<td align="center">3×3×128,1</td>
</tr>
<tr>
<td align="center">conv3b</td>
<td align="center">28×28×256</td>
<td align="center">28×28×256</td>
<td align="center">3×3×256,1</td>
</tr>
<tr>
<td align="center">pool3</td>
<td align="center">28×28×256</td>
<td align="center">14×14×256</td>
<td align="center">2×2×1,2</td>
</tr>
<tr>
<td align="center">conv4a</td>
<td align="center">14×14×256</td>
<td align="center">14×14×512</td>
<td align="center">3×3×256,1</td>
</tr>
<tr>
<td align="center">conv4b</td>
<td align="center">14×14×512</td>
<td align="center">14×14×512</td>
<td align="center">3×3×512,1</td>
</tr>
<tr>
<td align="center">pool4</td>
<td align="center">14×14×512</td>
<td align="center">7×7×512</td>
<td align="center">2×2×1,2</td>
</tr>
<tr>
<td align="center">fc1</td>
<td align="center">7×7×512</td>
<td align="center">1×1×256</td>
<td align="center">-</td>
</tr>
<tr>
<td align="center">fc2</td>
<td align="center">1×1×256</td>
<td align="center">1×1×136</td>
<td align="center">-</td>
</tr>
</tbody></table>
<p>Feed Forward NN的输入是经过“IMAGE TRANSFORM“之后得到的偏移量ΔStΔSt，它是在新特征空间下的偏移量，在经过偏移后再经过反变换Tt−1(⋅)Tt−1(⋅)，将其还原到原始空间。</p>
<p>关键点热度图的计算就是一个中心衰减，关键点处值最大，越远则值越小，公式如下：</p>
<p>H(x,y)=11+Si∈Tt(St−1)  ∥(x,y)−Si∥H(x,y)=1+minSi∈Tt(St−1)  ∥(x,y)−Si∥1</p>
<p>从fc1fc1层生成特种图的目的是人为给 CNN 增加上一阶段信息。总而言之，DAN 是一个级联思想的关键点检测方法，通过引入关键点热图作为补充，DAN 可以从整张图片进行提取特征，从而获得更为精确的定位。</p>
<h2 id="EmotionalDAN"><a href="#EmotionalDAN" class="headerlink" title="EmotionalDAN"></a>EmotionalDAN</h2><p>Author: NYY</p>
<p>Link: <a href="http://yoursite.com/2018/11/08/computer_version/face-keypoint-detection/">http://yoursite.com/2018/11/08/computer_version/face-keypoint-detection/</a></p>
<p>Copyright Notice: All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a>unless stating additionally.</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2019/08/24/OpenGL/Adobe%20Animiation%20Flow/">OpenGL/Adobe Animiation Flow</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-08-24</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Graphic/">Graphic</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Adobe/">Adobe</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Animation/">Animation</a></span><div class="content"><ol>
<li>Download Adobe &amp; Zii , in <a href="https://www.waitsun.com/adobe-zii-2019.html" target="_blank" rel="noopener">https://www.waitsun.com/adobe-zii-2019.html</a></li>
<li>install body movin</li>
<li><a href="https://github.com/airbnb/lottie-web#plugin-installation" target="_blank" rel="noopener">https://github.com/airbnb/lottie-web#plugin-installation</a> &amp; ZXP_installer : <a href="https://www.jianshu.com/p/9a1500571269" target="_blank" rel="noopener">https://www.jianshu.com/p/9a1500571269</a></li>
</ol>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2019/08/21/DevOps/2019-08-21-Homebrew-change-mirror-source/">DevOps/2019-08-21-Homebrew-change-mirror-source</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-08-21</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/DevOps/">DevOps</a></span><div class="content"><p>Homebrew是Mac的软件包管理器，我们可以通过它安装大多数开源软件。但是在使用brew update更新的时候竟然要等待很久。猜测可能是因为brew的官方源被墙或或者响应慢。于是想到的切换Homebrew的更新源的办法, 如果coding.net的源还是很慢的话， 也可以尝试其他的源。具体代码如下</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="comment">#cd to homebrew foler</span></span><br><span class="line">$ <span class="built_in">cd</span> <span class="string">"<span class="variable">$(brew --repo)</span>"</span>；</span><br><span class="line">$ <span class="comment">#check  git remote status</span></span><br><span class="line">$ git remote -v;</span><br><span class="line">https://github.com/Homebrew/homebrew.git</span><br><span class="line">$ <span class="comment">#update remote url with Coding.net</span></span><br><span class="line">$ git remote <span class="built_in">set</span>-url origin https://git.coding.net/homebrew/homebrew.git</span><br><span class="line">$ brew update</span><br></pre></td></tr></table></figure>



<p>　</p>
<p>brew镜像源</p>
<ul>
<li>中科大brew镜像源    <a href="http://mirrors.ustc.edu.cn/homebrew.git" target="_blank" rel="noopener">http://mirrors.ustc.edu.cn/homebrew.git</a></li>
<li>清华brew镜像源  <a href="http://mirrors.ustc.edu.cn/homebrew.git" target="_blank" rel="noopener">http://mirrors.ustc.edu.cn/homebrew.git</a></li>
</ul>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2019/08/04/CV%20notes/2019-08-04-Pose-Generation/">CV notes/2019-08-04-Pose-Generation</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-08-04</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Deep-Learning/">Deep-Learning</a><i class="fa fa-angle-right" aria-hidden="true"></i><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Deep-Learning/AI/">AI</a></span><div class="content"><p>(NOT MY NOTE, ref from colleaque’s)</p>
<h3 id="Deformable-GANs-for-Pose-based-Human-Image-Generation"><a href="#Deformable-GANs-for-Pose-based-Human-Image-Generation" class="headerlink" title="Deformable GANs for Pose-based Human Image Generation."></a>Deformable GANs for Pose-based Human Image Generation.</h3><p><a href="https://github.com/AliaksandrSiarohin/pose-gan" target="_blank" rel="noopener">https://github.com/AliaksandrSiarohin/pose-gan</a>.  cvpr2018</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g6hp3mtcvuj30rp0aq42d.jpg" alt=""></p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g6hp4oxs5gj30gd0adwh2.jpg" alt=""></p>
<h3 id="Progressive-Pose-Attention-for-Person-Image-Generation"><a href="#Progressive-Pose-Attention-for-Person-Image-Generation" class="headerlink" title="Progressive Pose Attention for Person Image Generation"></a>Progressive Pose Attention for Person Image Generation</h3><p><a href="https://github.com/tengteng95/Pose-Transfer" target="_blank" rel="noopener">https://github.com/tengteng95/Pose-Transfer</a>.  cvpr2019, </p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g6gthtcaz5j30o70b2wfd.jpg" alt="整体流程图"></p>
<p>整体结构中含有多个Pose  Attentional Block，其作用是对输入的image pathway和pose pathway按照Pose Mask进行更新，图中Mt即为Pose Mask，它引导网络将图片中人物的不同的部分按照目标姿态进行像素块迁移。</p>
<p>将最后一个Block中Image Pathway的数据经过解码网络，即得到了最终的生成图像。</p>
<h3 id="Unsupervised-Person-Image-Generation-with-Semantic-Parsing-Transformation"><a href="#Unsupervised-Person-Image-Generation-with-Semantic-Parsing-Transformation" class="headerlink" title="Unsupervised Person Image Generation with Semantic Parsing Transformation"></a>Unsupervised Person Image Generation with Semantic Parsing Transformation</h3><p><a href="https://github.com/SijieSong/person_generation_spt" target="_blank" rel="noopener">https://github.com/SijieSong/person_generation_spt</a>. cvpr2019</p>
<p>只有测试代码，测试输入还需要semantic parsing</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g6lf6q884ej311s0mb0vb.jpg" alt="">  </p>
<p>###Dense Intrinsic Appearance Flow for Human Pose Transfer</p>
<p><a href="https://github.com/ly015/intrinsic_flow" target="_blank" rel="noopener">https://github.com/ly015/intrinsic_flow</a>      cvpr2019</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g6m4tdrihzj31dk0lsgrt.jpg" alt=""></p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g6m4v9xer1j31bg0j40u9.jpg" alt=""></p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g6m4vecqfjj30o407udg5.jpg" alt=""></p>
<p>###Disentangled Person Image Generation</p>
<p><a href="https://github.com/charliememory/Disentangled-Person-Image-Generation" target="_blank" rel="noopener">https://github.com/charliememory/Disentangled-Person-Image-Generation</a> cvpr2018</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g6m4zyq9zhj31pa0n076v.jpg" alt=""></p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g6m4xvk2woj31l00l8di5.jpg" alt=""></p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2019/08/02/DevOps/2019-08-02-Docker-w-Pycharm/">DevOps/2019-08-02-Docker-w-Pycharm</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-08-02</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/DevOps/">DevOps</a></span><div class="content"><h3 id="docker-ssh配置"><a href="#docker-ssh配置" class="headerlink" title="docker ssh配置"></a>docker ssh配置</h3><h5 id="Concept"><a href="#Concept" class="headerlink" title="Concept"></a>Concept</h5><p>进行实验时用挂载的，让本地跟docker内的文件夹一样</p>
<p>#####新建docker container</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo nvidia-docker run -it -p [host_port]:[container_port](<span class="keyword">do</span> not use 8888) --name:[container_name] [image_name] -v [container_path]:[host_path] /bin/bash</span><br></pre></td></tr></table></figure>

<p>例如：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo nvidia-docker run -p 5592:5592 -p 5593:5593 -p 8022:22 --name=<span class="string">"liuzhen_tf"</span> -v ~/workspace/liuzhen/remote_workspace:/workspace/liuzhen/remote_workspace -it tensorflow/tensorflow:latest-gpu /bin/bash</span><br></pre></td></tr></table></figure>

<h5 id="配置ssh服务"><a href="#配置ssh服务" class="headerlink" title="配置ssh服务"></a>配置ssh服务</h5><p>在新建的容器内配置ssh，安装openssh-server</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ apt update</span><br><span class="line">$ apt install -y openssh-server</span><br></pre></td></tr></table></figure>

<p>设置ssh</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ mkdir /var/run/sshd</span><br><span class="line">$ <span class="built_in">echo</span> <span class="string">'root:passwd'</span> | chpasswd</span><br><span class="line"><span class="comment"># 这里使用你自己想设置的用户名和密码，passwd是此处设置的密码</span></span><br><span class="line">$ sed -i <span class="string">'s/PermitRootLogin prohibit-password/PermitRootLogin yes/'</span> /etc/ssh/sshd_config</span><br><span class="line">$ sed <span class="string">'s@session\s*required\s*pam_loginuid.so@session optional pam_loginuid.so@g'</span> -i /etc/pam.d/sshd</span><br><span class="line">$ <span class="built_in">echo</span> <span class="string">"export VISIBLE=now"</span> &gt;&gt; /etc/profile</span><br></pre></td></tr></table></figure>

<h5 id="重启ssh激活配置"><a href="#重启ssh激活配置" class="headerlink" title="重启ssh激活配置"></a>重启ssh激活配置</h5><p>(<u>最常见的问题</u>就是docker容器停了以后里面的SSH服务也会相应停止，因此当你发现自己某一天连不上的时候，记得去docker里重启一下ssh服务：)</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ service ssh restart</span><br></pre></td></tr></table></figure>

<p>在服务器（宿主机）上（不是服务器的docker里）测试刚刚新建docker容器中哪个端口转发到了服务器的22端口：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo docker port [your_container_name] 22</span><br><span class="line"><span class="comment"># 如果前面的配置生效了，你会看到如下输出</span></span><br><span class="line"><span class="comment"># 0.0.0.0:8022</span></span><br></pre></td></tr></table></figure>

<p>最后测试能否用SSH连接到远程docker：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ssh root@[your_host_ip] -p 8022</span><br><span class="line"><span class="comment"># 密码是你前面自己设置的</span></span><br></pre></td></tr></table></figure>

<h3 id="pycharm配置"><a href="#pycharm配置" class="headerlink" title="pycharm配置"></a>pycharm配置</h3><h5 id="打开本地代码"><a href="#打开本地代码" class="headerlink" title="打开本地代码"></a>打开本地代码</h5><p>打开PyCharm<code>Tools &gt; Deployment &gt; Configuration</code></p>
<p>#####新建服务器SFTP</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g6gtkcxaeaj315r0u03zh.jpg" alt="image-20190828175349739"></p>
<p>输入如下图配置，注意这里的端口是你刚刚设置的映射到容器22端口的宿主机中的端口，我这里使用的是8022，账号密码是你刚刚自己设置的，这里的Root Path设置一个远程docker容器里的路径。</p>
<p>在Mappings中配置路径，这里的路径是你本地存放代码的路径，与刚刚配置的<em>Root Path</em>相互映射（意思是Mapping里本机的路径映射到远程的Root Path）<em>，</em>方便以后在本地和远程docker中进行代码和其他文件同步。</p>
<p>#####配置远程解释器</p>
<p>打开PyCharm<code>Preferences &gt; Project Interpreter</code></p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g6gtlykktsj317a0u0q4w.jpg" alt="image-20190828190716498"></p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g6gtrxo163j307705et8o.jpg" alt=""></p>
<p>新建ssh Interpreter</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g6gtml7aj1j31d30u0aam.jpg" alt="image-20190828191025481"></p>
<h5 id="配置运行环境变量"><a href="#配置运行环境变量" class="headerlink" title="配置运行环境变量"></a>配置运行环境变量</h5><p><code>run &gt; edit confuriguration</code></p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g6gtmztm35j30r80sk758.jpg" alt="image-20190828191253964"></p>
<h3 id="docker-jupyter使用"><a href="#docker-jupyter使用" class="headerlink" title="docker jupyter使用"></a>docker jupyter使用</h3><p>jupyter-notebook的默认端口号是8888，映射几个外部端口，如前面的5592</p>
<p>更新apt</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apt-get update</span><br></pre></td></tr></table></figure>

<p>安装jupyter</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install jupyter</span><br></pre></td></tr></table></figure>

<p>(注意pip install jupyter notebook==5.7.5，高版本有bug)</p>
<p>配置jupyter</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">生成jupyter配置文件，这个会生成配置文件.jupyter/jupyter_notebook_config.py</span></span><br><span class="line">jupyter notebook --generate-config</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">使用ipython生成密码</span></span><br><span class="line">In [1]: from notebook.auth import passwd</span><br><span class="line">In [2]: passwd()</span><br><span class="line">Enter password: </span><br><span class="line">Verify password: </span><br><span class="line">Out[2]: 'sha1:38a5ecdf288b:c82dace8d3c7a212ec0bd49bbb99c9af3bae076e'</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">去配置文件.jupyter/jupyter_notebook_config.py中修改以下参数</span></span><br><span class="line">c.NotebookApp.ip='*'                          #绑定所有地址</span><br><span class="line">c.NotebookApp.password = u'sha1:38a5ecdf288b:c82dace8d3c7a212ec0bd49bbb99c9af3bae076e'</span><br><span class="line">c.NotebookApp.open_browser = False            #启动后是否在浏览器中自动打开</span><br><span class="line">c.NotebookApp.port =5592                      #指定一个访问端口，默认8888，注意和映射的docker端口对应</span><br><span class="line">c.NotebookApp.allow_remote_access = True</span><br><span class="line">c.NotebookApp.allow_root = True               #allow the user to run the notebook as root</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> jupyter notebook</span></span><br></pre></td></tr></table></figure>

</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2019/08/01/DevOps/2019-08-01-Rebase/">DevOps/2019-08-01-Rebase</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-08-01</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/DevOps/">DevOps</a></span><div class="content"><p>ref: <a href="https://git-scm.com/book/zh/v2/Git-分支-变基" target="_blank" rel="noopener">https://git-scm.com/book/zh/v2/Git-%E5%88%86%E6%94%AF-%E5%8F%98%E5%9F%BA</a></p>
<h2 id="变基"><a href="#变基" class="headerlink" title="变基"></a>变基</h2><p>在 Git 中整合来自不同分支的修改主要有两种方法：<code>merge</code> 以及 <code>rebase</code>。 在本节中我们将学习什么是“变基”，怎样使用“变基”，并将展示该操作的惊艳之处，以及指出在何种情况下你应避免使用它。</p>
<h3 id="变基的基本操作"><a href="#变基的基本操作" class="headerlink" title="变基的基本操作"></a>变基的基本操作</h3><p>请回顾之前在 <a href="https://git-scm.com/book/zh/v2/ch00/r_basic_merging" target="_blank" rel="noopener">分支的合并</a> 中的一个例子，你会看到开发任务分叉到两个不同分支，又各自提交了更新。</p>
<p><img src="https://git-scm.com/book/en/v2/images/basic-rebase-1.png" alt="分叉的提交历史。"></p>
<p>Figure 35. 分叉的提交历史</p>
<p>之前介绍过，整合分支最容易的方法是 <code>merge</code> 命令。 它会把两个分支的最新快照（<code>C3</code> 和 <code>C4</code>）以及二者最近的共同祖先（<code>C2</code>）进行三方合并，合并的结果是生成一个新的快照（并提交）。</p>
<p><img src="https://git-scm.com/book/en/v2/images/basic-rebase-2.png" alt="通过合并操作来整合分叉了的历史。"></p>
<p>Figure 36. 通过合并操作来整合分叉了的历史</p>
<p>其实，还有一种方法：你可以提取在 <code>C4</code> 中引入的补丁和修改，然后在 <code>C3</code> 的基础上应用一次。 在 Git 中，这种操作就叫做 <em>变基</em>。 你可以使用 <code>rebase</code> 命令将提交到某一分支上的所有修改都移至另一分支上，就好像“重新播放”一样。</p>
<p>在上面这个例子中，运行：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ git checkout experiment</span><br><span class="line">$ git rebase master</span><br><span class="line">First, rewinding head to replay your work on top of it...</span><br><span class="line">Applying: added staged <span class="built_in">command</span></span><br></pre></td></tr></table></figure>

<p>它的原理是首先找到这两个分支（即当前分支 <code>experiment</code>、变基操作的目标基底分支 <code>master</code>）的最近共同祖先 <code>C2</code>，然后对比当前分支相对于该祖先的历次提交，提取相应的修改并存为临时文件，然后将当前分支指向目标基底 <code>C3</code>, 最后以此将之前另存为临时文件的修改依序应用。（译注：写明了 commit id，以便理解，下同）</p>
<p><img src="https://git-scm.com/book/en/v2/images/basic-rebase-3.png" alt="将 `C4` 中的修改变基到 `C3` 上。"></p>
<p>Figure 37. 将 <code>C4</code> 中的修改变基到 <code>C3</code> 上</p>
<p>现在回到 <code>master</code> 分支，进行一次快进合并。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ git checkout master</span><br><span class="line">$ git merge experiment</span><br></pre></td></tr></table></figure>

<p><img src="https://git-scm.com/book/en/v2/images/basic-rebase-4.png" alt="master 分支的快进合并。"></p>
<p>Figure 38. master 分支的快进合并</p>
<p>此时，<code>C4&#39;</code> 指向的快照就和上面使用 <code>merge</code> 命令的例子中 <code>C5</code> 指向的快照一模一样了。 这两种整合方法的最终结果没有任何区别，但是变基使得提交历史更加整洁。 你在查看一个经过变基的分支的历史记录时会发现，尽管实际的开发工作是并行的，但它们看上去就像是串行的一样，提交历史是一条直线没有分叉。</p>
<p>一般我们这样做的目的是为了确保在向远程分支推送时能保持提交历史的整洁——例如向某个其他人维护的项目贡献代码时。 在这种情况下，你首先在自己的分支里进行开发，当开发完成时你需要先将你的代码变基到 <code>origin/master</code> 上，然后再向主项目提交修改。 这样的话，该项目的维护者就不再需要进行整合工作，只需要快进合并便可。</p>
<p>请注意，无论是通过变基，还是通过三方合并，整合的最终结果所指向的快照始终是一样的，只不过提交历史不同罢了。 变基是将一系列提交按照原有次序依次应用到另一分支上，而合并是把最终结果合在一起。</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2019/07/31/FFmpeg/2019-07-31-ffmepg-cmds-2/">FFmpeg/2019-07-31-ffmepg-cmds-2</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-07-31</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/ffmpeg/">ffmpeg</a></span><div class="content"><p>1.通过ffmpeg对AE直接导出的大文件压缩，(a.mov:16M，a.mp4:2.7M)，(b.mov:13M，a.mp4:1.6M)<br><code>ffmpeg -i a.mov -vcodec png a.mp4</code></p>
<p>2.对模板视频时长进行改变，(a.mp4:2.7M，a_fittime.mp4:2.6M)<br><code>ffmpeg -i a.mp4 -vf setpts=0.96*PTS -vcodec png a_fittime.mp4</code><br>模板6.25秒，素材6秒，经过上面的操作模板变成6.08秒</p>
<p>3.视频叠加，(intro_3_4.mp4:1.1M，a_fit_add.mp4:424k)<br><code>ffmpeg -i intro_3_4.mp4 -i a_fittime.mp4 -filter_complex overlay -strict -2 a_fit_add.mp4</code></p>
<p>4.Preset</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g7akqezzvtj30rv0j9dkl.jpg" alt="image"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ffoutputs += <span class="string">"[1:v]scale=if(gt(&#123;0&#125;/iw\, &#123;1&#125;/ih)\, -1\, &#123;0&#125;)"</span> \</span><br><span class="line">                       <span class="string">":if(gt(&#123;0&#125;/iw\, &#123;1&#125;/ih)\, &#123;1&#125;\, -1)[video];"</span>.format(dest_w, dest_h)</span><br></pre></td></tr></table></figure>
</div><hr></div><nav id="pagination"><div class="pagination"><a class="extend prev" rel="prev" href="/page/15/"><i class="fa fa-chevron-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/15/">15</a><span class="page-number current">16</span><a class="page-number" href="/page/17/">17</a><span class="space">&hellip;</span><a class="page-number" href="/page/19/">19</a><a class="extend next" rel="next" href="/page/17/"><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2013 - 2019 By Joe Huang</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody" target="_blank" rel="noopener"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_site_uv"><i class="fa fa-user"></i><span id="busuanzi_value_site_uv"></span><span></span></span><span class="footer-separator">|</span><span id="busuanzi_container_site_pv"><i class="fa fa-eye"></i><span id="busuanzi_value_site_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.1"></script><script src="/js/fancybox.js?version=1.6.1"></script><script src="/js/sidebar.js?version=1.6.1"></script><script src="/js/copy.js?version=1.6.1"></script><script src="/js/fireworks.js?version=1.6.1"></script><script src="/js/transition.js?version=1.6.1"></script><script src="/js/scroll.js?version=1.6.1"></script><script src="/js/head.js?version=1.6.1"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script></body></html>